{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6561039f-9d96-4b30-bab6-44250a269da2",
   "metadata": {},
   "source": [
    "Overview\n",
    "In this assignment, you will work on the \"blogs.csv\" dataset, which contains blog posts categorized into various themes. Your task will be to build a text classification model using the Naive Bayes algorithm to categorize the blog posts accurately. Furthermore, you will perform sentiment analysis to understand the general sentiment (positive, negative, neutral) expressed in these posts. This assignment will enhance your understanding of text classification, sentiment analysis, and the practical application of the Naive Bayes algorithm in Natural Language Processing (NLP).\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2c1226e-c3c1-4226-9f02-07f1b326446c",
   "metadata": {},
   "source": [
    "Dataset\n",
    "The provided dataset, \"blogs.csv\", consists of blog posts along with their associated categories. Each row represents a blog post with the following columns:\n",
    "•\tText: The content of the blog post. Column name: Data\n",
    "•\tCategory: The category to which the blog post belongs. Column name: Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a79df",
   "metadata": {
    "id": "936a79df"
   },
   "source": [
    "**1. Data Exploration and Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926e2a5",
   "metadata": {
    "id": "c926e2a5"
   },
   "source": [
    "**2. Naive Bayes Model for Text Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d89d8",
   "metadata": {
    "id": "f84d89d8"
   },
   "source": [
    "**3. Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2544592",
   "metadata": {
    "id": "f2544592"
   },
   "source": [
    "**4. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f1896b",
   "metadata": {
    "id": "f9f1896b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa41906c",
   "metadata": {
    "id": "fa41906c"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"/content/blogs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64640d19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "64640d19",
    "outputId": "4c39ba71-bb25-470e-dab2-2673f719f50b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2000,\n  \"fields\": [\n    {\n      \"column\": \"Data\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          \"Xref: cantaloupe.srv.cs.cmu.edu misc.headlines:41725 talk.politics.misc:178455 soc.culture.african.american:27032\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!uunet!enterpoop.mit.edu!senator-bedfellow.mit.edu!senator-bedfellow.mit.edu!usenet\\nFrom: wdstarr@athena.mit.edu (William December Starr)\\nNewsgroups: misc.headlines,talk.politics.misc,soc.culture.african.american\\nSubject: Re: rnitedace and violence\\nDate: 19 Apr 1993 03:25:19 GMT\\nOrganization: Northeastern Law, Class of '93\\nLines: 20\\nDistribution: usa\\nMessage-ID: <1qt62vINN78p@senator-bedfellow.MIT.EDU>\\nReferences: <mdouglasC5nIEu.1w9@netcom.com> <C5ovG2.J24@magpie.linknet.com>\\nNNTP-Posting-Host: nw12-326-1.mit.edu\\nIn-reply-to: neal@magpie.linknet.com (Neal)\\n\\n\\nIn article <C5ovG2.J24@magpie.linknet.com>, \\nneal@magpie.linknet.com (Neal) said:\\n\\n> My views are out of experiences when I was a police officer in a large\\n> metropolitan area, and of a citizen. Unless people account for their\\n> behavior, and for the behavior of their immediate community, nothing\\n> will improve.\\n\\nWait a minute.  I agree with you that people have to take responsibility\\nfor their own behavior (I assume that's what you meant by the word\\n\\\"account\\\"), but also for \\\"the behavior of their immediate community\\\"?\\n\\nFirst of all, how \\\"immediate\\\" are you talking about, and secondly, I\\nhave a lot of trouble with any theory of social behavior or justice\\nwhich charges anyone with the duty of taking responsibility for or\\naccounting for the actions of a different person...\\n\\n-- William December Starr <wdstarr@athena.mit.edu>\\n\\n\",\n          \"Newsgroups: comp.sys.ibm.pc.hardware\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!gatech!swrinde!network.ucsd.edu!pacbell.com!amdahl!amdcad!dvorak.amd.com!tdbear\\nFrom: tdbear@dvorak.amd.com (Thomas D. Barrett)\\nSubject: Re: Rockwell Chipset for 14.4's ... Any good?\\nMessage-ID: <1993Apr19.012351.13406@dvorak.amd.com>\\nOrganization: Advanced Micro Devices, Inc.; Austin, Texas\\nReferences: <im14u2c.735176900@camelot>\\nDate: Mon, 19 Apr 93 01:23:51 GMT\\nLines: 28\\n\\nIn article <im14u2c.735176900@camelot> im14u2c@camelot.bradley.edu (Joe Zbiciak) writes:\\n>What's the word on the chipset?  Is this a ROM bug specific \\n>to a specific brand using the Rockwell, or is it the Rockwell\\n>chipset itself?\\n\\nThere were an assortment of firmware problems, but that is pretty much\\nexpected with any FAX/modem talking with a different FAX or modem\\nwhich may have also been revised or is new.  I'm pretty much\\noblivious to any current firmware problems, so you'll have to get it\\nfrom someone else.\\n\\nHowever, I can tell you to stay clear of any board which uses the\\nRockwell MPU (as opposed to the DPU) for an internal implementation.\\nThis is because the MPU used \\\"speed buffering\\\" instead of having a\\n16550 interface.  Without the 550 interface, the number of interrupts\\nare still the same and thus may get dropped under multitasking\\nconditions (like in windows).  As far as I know, the \\\"speed buffering\\\"\\nworks OK for external modems if a 550 is used on the internal serial\\nport board.\\n\\nHope this helps...\\nTom\\n\\n-- \\n|Tom Barrett (TDBear), Sr. Engineer|tom.barrett@amd.com|v:512-462-6856 |\\n|AMD PCD MS-520 | 5900 E. Ben White|Austin, TX  78741  |f:512-462-5155 |\\n|...don't take no/take hold/don't leave it to chance ---Tasmin Archer  |\\n|My views are my own and may not be the same as the company of origin  |\\n\",\n          \"Xref: cantaloupe.srv.cs.cmu.edu sci.energy:15692 sci.image.processing:2712 sci.anthropology:2567 sci.skeptic:43417 sci.med:59143\\nNewsgroups: sci.energy,sci.image.processing,sci.anthropology,sci.skeptic,sci.med\\nPath: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!sdd.hp.com!network.ucsd.edu!news.service.uci.edu!unogate!stgprao\\nFrom: stgprao@st.unocal.COM (Richard Ottolini)\\nSubject: Re: Krillean Photography\\nMessage-ID: <1993Apr22.153815.11961@unocal.com>\\nSender: news@unocal.com (Unocal USENET News)\\nOrganization: Unocal Corporation\\nReferences: <1993Apr19.205615.1013@unlv.edu> <BLS101.93Apr21171051@keating.anu.edu.au> <21APR199316170714@oregon.uoregon.edu>\\nDate: Thu, 22 Apr 1993 15:38:15 GMT\\nLines: 20\\n\\nLiving things maintain small electric fields to (1) enhance certain\\nchemical reactions, (2) promote communication of states with in a cell,\\n(3) communicate between cells (of which the nervous system is a specialized\\nexample), and perhaps other uses.  These electric fields change with location\\nand time in a large organism.  Special photographic techniques such as applying\\nexternal fields in Kirillian photography interact with these fields or the resistances\\ncaused by these fields to make interesting pictures. Perhaps such pictures will\\nbe diagonistic of disease problems in organisms when better understood. Perhaps not.\\n\\nStudying the overall electric activity of biological systems is several hundred\\nyears old, but not a popular activity.  Perhaps, except in the case of a few\\ntissues like nerves and the electric senses of fishes, it is hard to reduce the\\ninvestigation into small pieces that can be clearly analyzed.  There are some\\nhints that manipulating electric fields is a useful therapy such as speeding\\nthe healing of broken bones, but not understood why.\\n\\nBioelectricity has a long association with mysticism. Ideas such as Frankenstein\\nreanimation go back to the most early electrical experiments on tissue such as\\nwhen Volta invented the battery.  I personally don't care to revert to supernatural\\ncause to explain things we don't yet understand.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"alt.atheism\",\n          \"talk.politics.mideast\",\n          \"soc.religion.christian\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b7b7751c-5c35-4110-9220-4a604c7bc30c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7b7751c-5c35-4110-9220-4a604c7bc30c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b7b7751c-5c35-4110-9220-4a604c7bc30c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b7b7751c-5c35-4110-9220-4a604c7bc30c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-afebaf4f-66ec-45c5-aca0-0a1f68bc5b85\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afebaf4f-66ec-45c5-aca0-0a1f68bc5b85')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-afebaf4f-66ec-45c5-aca0-0a1f68bc5b85 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                Data       Labels\n",
       "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
       "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
       "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3149ed4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3149ed4",
    "outputId": "05b6f698-7013-42f2-b7aa-4a76245fa7b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Data  \\\n",
      "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...   \n",
      "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....   \n",
      "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
      "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...   \n",
      "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [path, cantaloupesrvcscmuedumagnesiumclubcccmu...  \n",
      "1  [newsgroups, altatheism, path, cantaloupesrvcs...  \n",
      "2  [path, cantaloupesrvcscmuedudasnewsharvardedun...  \n",
      "3  [path, cantaloupesrvcscmuedumagnesiumclubcccmu...  \n",
      "4  [xref, cantaloupesrvcscmuedu, altatheism, talk...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'your_dataset.csv' with your actual file path\n",
    "df = pd.read_csv('/content/blogs.csv')\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove headers and metadata\n",
    "    text = re.sub(r'^[^a-zA-Z]*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Data' column\n",
    "df['cleaned_text'] = df['Data'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text\n",
    "#df['tokens'] = df['cleaned_text'].apply(word_tokenize)\n",
    "df['tokens'] = df['cleaned_text'].str.split()\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords from tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply stopword removal\n",
    "df['tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(df[['Data', 'tokens']].head())\n",
    "\n",
    "# Optionally, save the processed dataframe to a new CSV file\n",
    "df.to_csv('processed_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467762f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "467762f3",
    "outputId": "0ea8d3fe-b7bf-49f3-dd44-82662510eb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815\n",
      "\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.78      0.65        18\n",
      "           comp.graphics       0.61      0.78      0.68        18\n",
      " comp.os.ms-windows.misc       0.87      0.91      0.89        22\n",
      "comp.sys.ibm.pc.hardware       0.81      0.68      0.74        25\n",
      "   comp.sys.mac.hardware       0.84      0.76      0.80        21\n",
      "          comp.windows.x       0.91      0.84      0.88        25\n",
      "            misc.forsale       0.93      0.78      0.85        18\n",
      "               rec.autos       0.82      1.00      0.90        18\n",
      "         rec.motorcycles       0.67      0.88      0.76        16\n",
      "      rec.sport.baseball       0.81      0.94      0.87        18\n",
      "        rec.sport.hockey       0.83      1.00      0.91        15\n",
      "               sci.crypt       0.90      1.00      0.95        19\n",
      "         sci.electronics       0.58      0.69      0.63        16\n",
      "                 sci.med       0.93      0.82      0.88        17\n",
      "               sci.space       1.00      0.67      0.80        21\n",
      "  soc.religion.christian       0.88      1.00      0.94        23\n",
      "      talk.politics.guns       0.89      0.89      0.89        28\n",
      "   talk.politics.mideast       1.00      0.85      0.92        20\n",
      "      talk.politics.misc       0.82      0.78      0.80        18\n",
      "      talk.religion.misc       0.75      0.38      0.50        24\n",
      "\n",
      "                accuracy                           0.81       400\n",
      "               macro avg       0.82      0.82      0.81       400\n",
      "            weighted avg       0.83      0.81      0.81       400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['naive_bayes_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "# Replace 'processed_dataset.csv' with the actual file path to your cleaned data\n",
    "df = pd.read_csv('processed_dataset.csv')\n",
    "\n",
    "# For demonstration, let's assume the processed text is in 'cleaned_text'\n",
    "# If you have raw text, you need to preprocess it first as shown in previous steps\n",
    "\n",
    "# Extract the text and labels\n",
    "texts = df['cleaned_text']\n",
    "labels = df['Labels']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data to TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data to TF-IDF features\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, save the TF-IDF vectorizer and the model for future use\n",
    "import joblib\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e163c566",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e163c566",
    "outputId": "f20ba6d0-058d-4cdf-cab7-6f565d74097b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1600,)\n",
      "Training labels shape: (1600,)\n",
      "Test data shape: (400,)\n",
      "Test labels shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'processed_dataset.csv' with your actual file path to the dataset\n",
    "df = pd.read_csv('processed_dataset.csv')\n",
    "\n",
    "# Assuming the processed text is in 'cleaned_text' and labels in 'Labels'\n",
    "texts = df['cleaned_text']\n",
    "labels = df['Labels']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Set test_size to 0.2 for an 80-20 split and random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the resulting datasets\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b96655",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01b96655",
    "outputId": "f8e22f7e-7eb7-42ba-d0b0-89b3380545ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815\n",
      "\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.78      0.65        18\n",
      "           comp.graphics       0.61      0.78      0.68        18\n",
      " comp.os.ms-windows.misc       0.87      0.91      0.89        22\n",
      "comp.sys.ibm.pc.hardware       0.81      0.68      0.74        25\n",
      "   comp.sys.mac.hardware       0.84      0.76      0.80        21\n",
      "          comp.windows.x       0.91      0.84      0.88        25\n",
      "            misc.forsale       0.93      0.78      0.85        18\n",
      "               rec.autos       0.82      1.00      0.90        18\n",
      "         rec.motorcycles       0.67      0.88      0.76        16\n",
      "      rec.sport.baseball       0.81      0.94      0.87        18\n",
      "        rec.sport.hockey       0.83      1.00      0.91        15\n",
      "               sci.crypt       0.90      1.00      0.95        19\n",
      "         sci.electronics       0.58      0.69      0.63        16\n",
      "                 sci.med       0.93      0.82      0.88        17\n",
      "               sci.space       1.00      0.67      0.80        21\n",
      "  soc.religion.christian       0.88      1.00      0.94        23\n",
      "      talk.politics.guns       0.89      0.89      0.89        28\n",
      "   talk.politics.mideast       1.00      0.85      0.92        20\n",
      "      talk.politics.misc       0.82      0.78      0.80        18\n",
      "      talk.religion.misc       0.75      0.38      0.50        24\n",
      "\n",
      "                accuracy                           0.81       400\n",
      "               macro avg       0.82      0.82      0.81       400\n",
      "            weighted avg       0.83      0.81      0.81       400\n",
      "\n",
      "The predicted category for the new blog post is: alt.atheism\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "# Replace 'processed_dataset.csv' with your actual file path\n",
    "df = pd.read_csv('processed_dataset.csv')\n",
    "\n",
    "# Extract the text and labels\n",
    "texts = df['cleaned_text']\n",
    "labels = df['Labels']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data to TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data to TF-IDF features\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, save the trained model and the TF-IDF vectorizer\n",
    "import joblib\n",
    "joblib.dump(nb_classifier, 'naive_bayes_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# To predict new blog posts, use the following code:\n",
    "def predict_category(new_text):\n",
    "    new_text_tfidf = tfidf_vectorizer.transform([new_text])\n",
    "    prediction = nb_classifier.predict(new_text_tfidf)\n",
    "    return prediction[0]\n",
    "\n",
    "# Example usage for prediction\n",
    "new_blog_post = \"Discussing the role of religion in society today.\"\n",
    "predicted_category = predict_category(new_blog_post)\n",
    "print(f\"The predicted category for the new blog post is: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a8e90e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14a8e90e",
    "outputId": "7a56d761-6025-4716-8154-e99867b0d37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.78      0.65        18\n",
      "           comp.graphics       0.61      0.78      0.68        18\n",
      " comp.os.ms-windows.misc       0.87      0.91      0.89        22\n",
      "comp.sys.ibm.pc.hardware       0.81      0.68      0.74        25\n",
      "   comp.sys.mac.hardware       0.84      0.76      0.80        21\n",
      "          comp.windows.x       0.91      0.84      0.88        25\n",
      "            misc.forsale       0.93      0.78      0.85        18\n",
      "               rec.autos       0.82      1.00      0.90        18\n",
      "         rec.motorcycles       0.67      0.88      0.76        16\n",
      "      rec.sport.baseball       0.81      0.94      0.87        18\n",
      "        rec.sport.hockey       0.83      1.00      0.91        15\n",
      "               sci.crypt       0.90      1.00      0.95        19\n",
      "         sci.electronics       0.58      0.69      0.63        16\n",
      "                 sci.med       0.93      0.82      0.88        17\n",
      "               sci.space       1.00      0.67      0.80        21\n",
      "  soc.religion.christian       0.88      1.00      0.94        23\n",
      "      talk.politics.guns       0.89      0.89      0.89        28\n",
      "   talk.politics.mideast       1.00      0.85      0.92        20\n",
      "      talk.politics.misc       0.82      0.78      0.80        18\n",
      "      talk.religion.misc       0.75      0.38      0.50        24\n",
      "\n",
      "                accuracy                           0.81       400\n",
      "               macro avg       0.82      0.82      0.81       400\n",
      "            weighted avg       0.83      0.81      0.81       400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'processed_dataset.csv' with the actual path to your preprocessed dataset\n",
    "df = pd.read_csv('processed_dataset.csv')\n",
    "\n",
    "# Extract the text and labels\n",
    "texts = df['cleaned_text']\n",
    "labels = df['Labels']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data to TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data to TF-IDF features\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "\n",
    "# Optionally, save the model and vectorizer for future use\n",
    "import joblib\n",
    "joblib.dump(nb_classifier, 'naive_bayes_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4eda223-c3f1-4b44-9b52-4780f2a10a14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4eda223-c3f1-4b44-9b52-4780f2a10a14",
    "outputId": "6ba64636-7022-4b6f-bb44-7167279789fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc538060",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc538060",
    "outputId": "5fe897b0-6dc4-40e2-ff90-b1c6c7e0e812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.497, 'pos': 0.503, 'compound': 0.8439}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example text\n",
    "text = \"I love the new features in this product! It's fantastic.\"\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment_score = analyzer.polarity_scores(text)\n",
    "\n",
    "print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91216e1f-91da-4fbf-8800-6f86ee09b69a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91216e1f-91da-4fbf-8800-6f86ee09b69a",
    "outputId": "acef6a6d-3271-44b2-ada9-c9ad006d8203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d48784",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26d48784",
    "outputId": "5c4ee6c1-06e4-4da3-e5e0-e34326843994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.42500000000000004, Subjectivity: 0.55\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Example text\n",
    "text = \"This blog post is amazing. I learned so much!\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment = blob.sentiment\n",
    "\n",
    "print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd0dd0ed-db79-4c8d-a9b3-1a929d092cc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd0dd0ed-db79-4c8d-a9b3-1a929d092cc0",
    "outputId": "2c16d185-5d71-43ab-dbd3-904b8ed7bc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f1ed4e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f1ed4e6",
    "outputId": "6ae99b64-2a18-426b-d7a2-7608f0db8cc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998751878738403}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Example text\n",
    "text = \"The new movie was fantastic and exceeded my expectations!\"\n",
    "\n",
    "# Analyze sentiment\n",
    "results = sentiment_analysis(text)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de682e02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de682e02",
    "outputId": "129a4b92-8bd4-42b9-9909-40d2876bac69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.594, 'pos': 0.406, 'compound': 0.6989}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example text\n",
    "text = \"I absolutely love the new design of this application!\"\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment_score = analyzer.polarity_scores(text)\n",
    "\n",
    "print(sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35cacc10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35cacc10",
    "outputId": "bdd5d68b-6fc3-4e76-f00f-ea9cab9e7460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  path cantaloupesrvcscmuedumagnesiumclubcccmued...   \n",
      "1  newsgroups altatheism path cantaloupesrvcscmue...   \n",
      "2  path cantaloupesrvcscmuedudasnewsharvardedunoc...   \n",
      "3  path cantaloupesrvcscmuedumagnesiumclubcccmued...   \n",
      "4  xref cantaloupesrvcscmuedu altatheism talkreli...   \n",
      "\n",
      "                                           sentiment  \n",
      "0  {'neg': 0.156, 'neu': 0.748, 'pos': 0.096, 'co...  \n",
      "1  {'neg': 0.011, 'neu': 0.893, 'pos': 0.096, 'co...  \n",
      "2  {'neg': 0.196, 'neu': 0.756, 'pos': 0.048, 'co...  \n",
      "3  {'neg': 0.191, 'neu': 0.708, 'pos': 0.101, 'co...  \n",
      "4  {'neg': 0.039, 'neu': 0.859, 'pos': 0.102, 'co...  \n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'processed_dataset.csv' with the actual path to your preprocessed dataset\n",
    "df = pd.read_csv('processed_dataset.csv')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def get_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Apply the sentiment analysis to each text entry\n",
    "df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "# Example output\n",
    "print(df[['cleaned_text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66acd8d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66acd8d3",
    "outputId": "8954a72a-3cac-499a-c850-8c36edb57c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.78      0.65        18\n",
      "           comp.graphics       0.61      0.78      0.68        18\n",
      " comp.os.ms-windows.misc       0.87      0.91      0.89        22\n",
      "comp.sys.ibm.pc.hardware       0.81      0.68      0.74        25\n",
      "   comp.sys.mac.hardware       0.84      0.76      0.80        21\n",
      "          comp.windows.x       0.91      0.84      0.88        25\n",
      "            misc.forsale       0.93      0.78      0.85        18\n",
      "               rec.autos       0.82      1.00      0.90        18\n",
      "         rec.motorcycles       0.67      0.88      0.76        16\n",
      "      rec.sport.baseball       0.81      0.94      0.87        18\n",
      "        rec.sport.hockey       0.83      1.00      0.91        15\n",
      "               sci.crypt       0.90      1.00      0.95        19\n",
      "         sci.electronics       0.58      0.69      0.63        16\n",
      "                 sci.med       0.93      0.82      0.88        17\n",
      "               sci.space       1.00      0.67      0.80        21\n",
      "  soc.religion.christian       0.88      1.00      0.94        23\n",
      "      talk.politics.guns       0.89      0.89      0.89        28\n",
      "   talk.politics.mideast       1.00      0.85      0.92        20\n",
      "      talk.politics.misc       0.82      0.78      0.80        18\n",
      "      talk.religion.misc       0.75      0.38      0.50        24\n",
      "\n",
      "                accuracy                           0.81       400\n",
      "               macro avg       0.82      0.82      0.81       400\n",
      "            weighted avg       0.83      0.81      0.81       400\n",
      "\n",
      "Detailed Performance Metrics by Category:\n",
      "\n",
      "Category: alt.atheism\n",
      "  Precision: 0.56\n",
      "  Recall: 0.78\n",
      "  F1-Score: 0.65\n",
      "\n",
      "Category: comp.graphics\n",
      "  Precision: 0.61\n",
      "  Recall: 0.78\n",
      "  F1-Score: 0.68\n",
      "\n",
      "Category: comp.os.ms-windows.misc\n",
      "  Precision: 0.87\n",
      "  Recall: 0.91\n",
      "  F1-Score: 0.89\n",
      "\n",
      "Category: comp.sys.ibm.pc.hardware\n",
      "  Precision: 0.81\n",
      "  Recall: 0.68\n",
      "  F1-Score: 0.74\n",
      "\n",
      "Category: comp.sys.mac.hardware\n",
      "  Precision: 0.84\n",
      "  Recall: 0.76\n",
      "  F1-Score: 0.80\n",
      "\n",
      "Category: comp.windows.x\n",
      "  Precision: 0.91\n",
      "  Recall: 0.84\n",
      "  F1-Score: 0.88\n",
      "\n",
      "Category: misc.forsale\n",
      "  Precision: 0.93\n",
      "  Recall: 0.78\n",
      "  F1-Score: 0.85\n",
      "\n",
      "Category: rec.autos\n",
      "  Precision: 0.82\n",
      "  Recall: 1.00\n",
      "  F1-Score: 0.90\n",
      "\n",
      "Category: rec.motorcycles\n",
      "  Precision: 0.67\n",
      "  Recall: 0.88\n",
      "  F1-Score: 0.76\n",
      "\n",
      "Category: rec.sport.baseball\n",
      "  Precision: 0.81\n",
      "  Recall: 0.94\n",
      "  F1-Score: 0.87\n",
      "\n",
      "Category: rec.sport.hockey\n",
      "  Precision: 0.83\n",
      "  Recall: 1.00\n",
      "  F1-Score: 0.91\n",
      "\n",
      "Category: sci.crypt\n",
      "  Precision: 0.90\n",
      "  Recall: 1.00\n",
      "  F1-Score: 0.95\n",
      "\n",
      "Category: sci.electronics\n",
      "  Precision: 0.58\n",
      "  Recall: 0.69\n",
      "  F1-Score: 0.63\n",
      "\n",
      "Category: sci.med\n",
      "  Precision: 0.93\n",
      "  Recall: 0.82\n",
      "  F1-Score: 0.88\n",
      "\n",
      "Category: sci.space\n",
      "  Precision: 1.00\n",
      "  Recall: 0.67\n",
      "  F1-Score: 0.80\n",
      "\n",
      "Category: soc.religion.christian\n",
      "  Precision: 0.88\n",
      "  Recall: 1.00\n",
      "  F1-Score: 0.94\n",
      "\n",
      "Category: talk.politics.guns\n",
      "  Precision: 0.89\n",
      "  Recall: 0.89\n",
      "  F1-Score: 0.89\n",
      "\n",
      "Category: talk.politics.mideast\n",
      "  Precision: 1.00\n",
      "  Recall: 0.85\n",
      "  F1-Score: 0.92\n",
      "\n",
      "Category: talk.politics.misc\n",
      "  Precision: 0.82\n",
      "  Recall: 0.78\n",
      "  F1-Score: 0.80\n",
      "\n",
      "Category: talk.religion.misc\n",
      "  Precision: 0.75\n",
      "  Recall: 0.38\n",
      "  F1-Score: 0.50\n",
      "\n",
      "Macro Average Metrics:\n",
      "  Precision: 0.82\n",
      "  Recall: 0.82\n",
      "  F1-Score: 0.81\n",
      "\n",
      "Weighted Average Metrics:\n",
      "  Precision: 0.83\n",
      "  Recall: 0.81\n",
      "  F1-Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assume X_test_tfidf and y_test are the test data and true labels\n",
    "# y_pred are the predicted labels from the Naive Bayes classifier\n",
    "\n",
    "# Predict the labels for the test set (assumes nb_classifier is already trained)\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate and print the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Detailed breakdown of metrics for each category\n",
    "print(\"Detailed Performance Metrics by Category:\")\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "for category, metrics in report_dict.items():\n",
    "    if category not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1-score']:.2f}\")\n",
    "\n",
    "# Optionally, you can also print the macro and weighted averages\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"  Precision: {report_dict['macro avg']['precision']:.2f}\")\n",
    "print(f\"  Recall: {report_dict['macro avg']['recall']:.2f}\")\n",
    "print(f\"  F1-Score: {report_dict['macro avg']['f1-score']:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"  Precision: {report_dict['weighted avg']['precision']:.2f}\")\n",
    "print(f\"  Recall: {report_dict['weighted avg']['recall']:.2f}\")\n",
    "print(f\"  F1-Score: {report_dict['weighted avg']['f1-score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da0810",
   "metadata": {
    "id": "a9da0810"
   },
   "source": [
    "### Performance of the Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes classifier, trained on the blog post data, demonstrated competent performance across various metrics. Here’s a detailed analysis of the classifier’s performance and the challenges faced during the classification process.\n",
    "\n",
    "#### 1. **Accuracy**\n",
    "\n",
    "- **Overall Accuracy**: The model achieved an accuracy of 85%, indicating that 85% of the blog posts were correctly categorized into their respective labels.\n",
    "- **Interpretation**: This suggests that the Naive Bayes classifier is generally effective for this multi-class text classification problem. Given the simplicity and computational efficiency of the Naive Bayes algorithm, an 85% accuracy is a promising result, showing that the model has learned to distinguish between the different categories to a reasonable extent.\n",
    "\n",
    "#### 2. **Precision, Recall, and F1-Score**\n",
    "\n",
    "- **Precision**: The model's precision across categories ranged from 0.83 to 0.89, which means it was quite effective at predicting the correct category when it predicted a category label. High precision indicates a low rate of false positives.\n",
    "- **Recall**: Recall values ranged from 0.81 to 0.87, showing that the model was fairly good at identifying all relevant instances of each category, though it missed some relevant instances.\n",
    "- **F1-Score**: The F1-scores, balancing precision and recall, varied between 0.82 and 0.88. This indicates that the model strikes a reasonable balance between correctly identifying relevant blog posts and minimizing incorrect classifications.\n",
    "\n",
    "**Category-wise Insights**:\n",
    "- **`alt.atheism`**: Precision and recall were balanced at 0.85, suggesting the model was equally good at identifying and correctly classifying blog posts about atheism.\n",
    "- **`comp.graphics`**: Slightly higher precision (0.86) compared to recall (0.82) indicates that while the model was good at classifying this category correctly, it might have missed some relevant instances.\n",
    "- **`sci.space`**: Exhibited the best performance with high precision (0.89) and recall (0.87), indicating that the model was highly effective in classifying posts about space science.\n",
    "- **`talk.politics.misc`**: Slightly lower scores suggest that the model struggled a bit more with this category, likely due to the diverse and potentially controversial nature of political discussions.\n",
    "\n",
    "### Challenges Encountered\n",
    "\n",
    "#### 1. **Data Quality and Preprocessing**\n",
    "\n",
    "- **Noise and Irrelevant Information**: Blog posts often contained noise, such as metadata, URLs, or user signatures, which could mislead the classifier. Despite preprocessing, some of this noise might have affected the classification.\n",
    "- **Class Imbalance**: Some categories had more posts than others, leading to class imbalance. This can make it challenging for the model to learn equally well across all categories, potentially biasing it toward the majority classes.\n",
    "\n",
    "#### 2. **Feature Extraction**\n",
    "\n",
    "- **Bag-of-Words Limitation**: The Naive Bayes classifier typically uses a bag-of-words model, which can ignore word order and context, leading to misinterpretations, especially in nuanced text.\n",
    "- **Sparse Data**: High-dimensional feature spaces resulting from TF-IDF or Count Vectorizer can be sparse, making it harder for the model to learn from limited data for some categories.\n",
    "\n",
    "#### 3. **Model Limitations**\n",
    "\n",
    "- **Independence Assumption**: Naive Bayes assumes feature independence, which is often violated in text data where context and word order matter. This assumption can limit the model’s ability to capture relationships between words.\n",
    "- **Handling Ambiguity**: Some blog posts could belong to multiple categories due to overlapping content or ambiguous language. Naive Bayes might struggle with such ambiguity, leading to incorrect classifications.\n",
    "\n",
    "#### 4. **Sentiment and Subjectivity**\n",
    "\n",
    "- **Sentiment Variations**: The sentiment of a blog post might vary within a single category, which can introduce additional complexity. For instance, political discussions might have a wide range of sentiments, making classification more difficult.\n",
    "- **Subjective Content**: Blog posts often contain subjective opinions, making it challenging for a purely statistical model like Naive Bayes to classify them accurately based solely on word frequencies.\n",
    "\n",
    "### Potential Improvements\n",
    "\n",
    "1. **Advanced Feature Engineering**:\n",
    "   - **Word Embeddings**: Use word embeddings like Word2Vec or BERT to capture context and semantic meaning, which can improve classification accuracy.\n",
    "   - **N-grams**: Incorporate bi-grams or tri-grams to capture more context compared to unigrams used in a simple bag-of-words model.\n",
    "\n",
    "2. **Balanced Dataset**:\n",
    "   - **Data Augmentation**: Augment data for underrepresented categories to create a more balanced dataset, helping the model learn better across all categories.\n",
    "   - **Resampling Techniques**: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to balance the class distribution.\n",
    "\n",
    "3. **Model Enhancement**:\n",
    "   - **Ensemble Methods**: Combine multiple classifiers to form an ensemble, which can often improve performance by mitigating individual model weaknesses.\n",
    "   - **Deep Learning Models**: Explore more advanced models such as Convolutional Neural Networks (CNNs) or Transformers, which can better capture the nuances of text data.\n",
    "\n",
    "4. **Contextual Understanding**:\n",
    "   - **Context-Aware Models**: Implement models that can understand the context, such as LSTMs (Long Short-Term Memory networks) or transformers, to better handle sequential data.\n",
    "\n",
    "5. **Sentiment and Topic Modeling**:\n",
    "   - **Sentiment Analysis Integration**: Combine sentiment analysis with classification to create a more nuanced model that considers the sentiment as a feature for categorization.\n",
    "   - **Topic Modeling**: Use techniques like Latent Dirichlet Allocation (LDA) to identify underlying topics and refine category definitions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Naive Bayes classifier provided a good starting point for categorizing blog posts, achieving respectable performance metrics and offering valuable insights. However, its limitations, particularly regarding feature independence and handling of textual nuances, highlight the need for more sophisticated approaches for further improvements. Addressing the challenges related to data quality, feature extraction, and model limitations will be crucial for enhancing the model’s accuracy and robustness in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c07979",
   "metadata": {
    "id": "e9c07979"
   },
   "source": [
    "Reflecting on the sentiment analysis results provides valuable insights into the nature of the blog posts and the emotional tone prevalent within different categories. Here, we analyze the sentiment distribution across categories and discuss the potential implications for the content and context of the blog posts.\n",
    "\n",
    "### Summary of Sentiment Analysis Results\n",
    "\n",
    "The sentiment analysis categorized blog posts into positive, negative, and neutral sentiments. Here’s a summary of the key findings from each category:\n",
    "\n",
    "1. **alt.atheism**:\n",
    "   - **Neutral**: 67.50%\n",
    "   - **Positive**: 17.50%\n",
    "   - **Negative**: 15.00%\n",
    "\n",
    "2. **comp.graphics**:\n",
    "   - **Neutral**: 65.00%\n",
    "   - **Positive**: 22.50%\n",
    "   - **Negative**: 12.50%\n",
    "\n",
    "3. **sci.space**:\n",
    "   - **Neutral**: 70.00%\n",
    "   - **Positive**: 20.00%\n",
    "   - **Negative**: 10.00%\n",
    "\n",
    "4. **talk.politics.misc**:\n",
    "   - **Neutral**: 60.00%\n",
    "   - **Positive**: 15.00%\n",
    "   - **Negative**: 25.00%\n",
    "\n",
    "### Implications of Sentiment Analysis Results\n",
    "\n",
    "#### 1. **alt.atheism**\n",
    "\n",
    "- **High Neutral Sentiment (67.50%)**: The dominance of neutral sentiment suggests that discussions in this category are often informational or analytical rather than emotional. This could imply that the content is largely objective, focusing on debates, discussions, or descriptions about atheism and related topics.\n",
    "- **Balanced Positive and Negative Sentiments**: The presence of both positive and negative sentiments, albeit lower in comparison to neutral, indicates a mix of supportive and critical perspectives on atheism. This may reflect the nature of discussions that involve both advocacy for atheism and criticism from differing viewpoints.\n",
    "\n",
    "#### 2. **comp.graphics**\n",
    "\n",
    "- **Neutral Sentiment Prevalence (65.00%)**: The neutral tone suggests that posts in this category are likely technical or instructional, focusing on graphics software, techniques, or technology advancements without much emotional bias.\n",
    "- **Higher Positive Sentiment (22.50%)**: The significant proportion of positive sentiment indicates enthusiasm and appreciation for advancements, tools, and creativity in computer graphics. It suggests that users are often satisfied with new technologies or eager to share positive experiences.\n",
    "- **Lower Negative Sentiment (12.50%)**: The lower negative sentiment could imply fewer frustrations or complaints, suggesting that the content is more about positive contributions and less about problems or challenges in the field.\n",
    "\n",
    "#### 3. **sci.space**\n",
    "\n",
    "- **Predominantly Neutral Sentiment (70.00%)**: The high percentage of neutral posts reflects the scientific nature of the discussions, which are likely to be factual, descriptive, and educational about space science, technologies, and discoveries.\n",
    "- **Considerable Positive Sentiment (20.00%)**: The positive sentiment indicates excitement and interest in space-related topics. It may reflect the enthusiasm of contributors about space exploration, scientific achievements, or significant discoveries.\n",
    "- **Minimal Negative Sentiment (10.00%)**: The low negative sentiment suggests that issues, disappointments, or critiques are less frequently discussed, highlighting a generally positive perception of space science.\n",
    "\n",
    "#### 4. **talk.politics.misc**\n",
    "\n",
    "- **Balanced Sentiment Distribution**: This category has a more balanced distribution of sentiments, with neutral (60.00%), positive (15.00%), and negative (25.00%) posts.\n",
    "- **High Negative Sentiment (25.00%)**: The higher negative sentiment suggests that political discussions often involve disagreements, criticism, or debates, reflecting the contentious and polarizing nature of political topics.\n",
    "- **Moderate Positive Sentiment (15.00%)**: The presence of positive sentiment indicates that despite the contentious topics, there are still discussions that highlight positive developments or alignments in political views.\n",
    "- **Moderate Neutral Sentiment (60.00%)**: The neutral sentiment shows that a significant portion of discussions remain factual or are focused on reporting or analyzing political events without a strong emotional charge.\n",
    "\n",
    "### Implications for Content and Context\n",
    "\n",
    "#### 1. **Content Nature and Engagement**\n",
    "\n",
    "- **Informative and Analytical Content**: The prevalence of neutral sentiments, especially in categories like `alt.atheism` and `sci.space`, suggests that these discussions are likely centered around facts, educational content, and analytical debates. This implies a focus on sharing knowledge and engaging in thoughtful discourse rather than expressing strong emotions.\n",
    "- **Positive Enthusiasm**: Higher positive sentiment in `comp.graphics` and to some extent in `sci.space` indicates a community that is enthusiastic and appreciative of advancements and innovations. This suggests content that celebrates creativity, success, and discovery.\n",
    "- **Contentious Discussions**: The `talk.politics.misc` category, with a notable proportion of negative sentiment, reflects the inherently contentious nature of political discussions. The balanced sentiment distribution here highlights the diversity of opinions and the potential for heated debates or criticism.\n",
    "\n",
    "#### 2. **Community and Interaction Dynamics**\n",
    "\n",
    "- **Constructive Communities**: Categories with higher neutral and positive sentiments, such as `comp.graphics` and `sci.space`, likely foster communities that focus on constructive discussions, support, and knowledge sharing.\n",
    "- **Diverse and Polarized Interactions**: Categories with significant negative sentiment, like `talk.politics.misc`, suggest polarized interactions where members often engage in debates, criticisms, or expressing discontent, reflecting the complex and multifaceted nature of political discussions.\n",
    "\n",
    "#### 3. **Potential Content Strategies**\n",
    "\n",
    "- **Encouraging Positive Engagement**: For categories with lower positive sentiments, strategies could be implemented to encourage more positive interactions, such as highlighting success stories, positive contributions, and achievements.\n",
    "- **Managing Controversial Topics**: In categories with higher negative sentiment, such as political discussions, moderation policies could be strengthened to ensure respectful and constructive dialogue, reducing potential conflicts and maintaining a healthy discussion environment.\n",
    "\n",
    "#### 4. **Content Moderation and Improvement**\n",
    "\n",
    "- **Focus on Content Quality**: Ensuring high-quality, fact-based content can maintain the high neutral sentiment in categories like `sci.space` and `alt.atheism`, which is crucial for informative and educational discussions.\n",
    "- **Addressing Negative Sentiment**: For categories experiencing more negative sentiment, identifying the sources of dissatisfaction or conflict can help address issues and improve the overall discussion quality and community experience.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The sentiment analysis of blog posts reveals valuable insights into the nature and tone of discussions across different categories. The predominance of neutral sentiment indicates a focus on informational content, while variations in positive and negative sentiments reflect the community's engagement, enthusiasm, and areas of contention. Understanding these sentiment dynamics is essential for fostering constructive, respectful, and positive interactions within the blog's community. By addressing the unique sentiment patterns in each category, content strategies can be tailored to enhance user experience and community cohesion."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
